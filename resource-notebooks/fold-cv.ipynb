{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98c75399",
   "metadata": {
    "papermill": {
     "duration": 0.007624,
     "end_time": "2023-11-05T12:19:09.308396",
     "exception": false,
     "start_time": "2023-11-05T12:19:09.300772",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 🧹 Importing necessary libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d5c8e4",
   "metadata": {
    "papermill": {
     "duration": 0.006865,
     "end_time": "2023-11-05T12:19:09.322366",
     "exception": false,
     "start_time": "2023-11-05T12:19:09.315501",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Enhanced Trading Predictions with Advanced Purging CV 🌐⏳📈\n",
    "\n",
    "Welcome to this evolved Jupyter notebook dedicated to the Optiver - Trading at the Close challenge. Building upon the profound insights from previous contributors, this notebook is aimed at propelling predictive accuracy with an advanced cross-validation strategy crafted for time series data in financial markets.\n",
    "\n",
    "### Special Thanks and Recognition 🎉🙏\n",
    "Immense appreciation goes to the foundational work of the following creators, which has been pivotal for the enhancements made in this version:\n",
    "- A hearty thanks to Angle for the initial inspiration and comprehensive framework provided in the [Optiver: Robust Best Single Model](https://www.kaggle.com/code/lblhandsome/optiver-robust-best-single-model/notebook).\n",
    "- Kudos to Zulqarnainali for insightful comments and parameter refinements, further explored in [Explained Single Model - Optiver](https://www.kaggle.com/code/zulqarnainali/explained-singel-model-optiver).\n",
    "\n",
    "Both their contributions have significantly influenced the methodologies applied here. 🌟\n",
    "\n",
    "Explore and engage with more projects, your feedback is invaluable:\n",
    "👉 [Visit my Profile](https://www.kaggle.com/verracodeguacas) 👈\n",
    "\n",
    "A huge thank you to everyone engaging with this notebook! If you find it helpful, a thumbs-up 👍 would be much appreciated!\n",
    "\n",
    "## My Journey Forward 🔍🚀\n",
    "This notebook encapsulates the evolution of learning, featuring key enhancements for:\n",
    "- Sophisticated data preparation techniques 🗄️🔍\n",
    "- Advanced feature engineering to decrypt market complexities 🛠️📊\n",
    "- A tailored purging cross-validation strategy accommodating temporal data nuances 📅➡️\n",
    "- Focused model training aimed at achieving high-precision predictions 🎯🧠\n",
    "- Strategic submission planning in line with the competition's goals 🎲📤\n",
    "\n",
    "## Structured Approach to Advanced Predictions 📘📐\n",
    "We adhere to a systematic and analytical approach:\n",
    "1. **Data Preparation**: Stringent preprocessing for a robust modeling foundation.\n",
    "2. **Feature Engineering**: Deliberate crafting of features that reflect the dynamics of the market.\n",
    "3. **Purging Cross-Validation**: Implementing an innovative CV strategy that honors the temporal aspect of financial data.\n",
    "4. **Model Training**: Concentrating on models that excel in precision and generalizability.\n",
    "5. **Prediction & Strategy**: Crafting a well-thought-out plan for prediction and competition submission.\n",
    "\n",
    "## Utilization Guide 🗝️🛠️\n",
    "Maximize the value of this notebook by:\n",
    "1. Ensuring the competition data and computational setup are ready.\n",
    "2. Proceeding through the notebook in sequential order, from data prep to final predictions.\n",
    "3. Tailoring and enhancing the code to fit your unique analysis and experimentation needs.\n",
    "\n",
    "## Heartfelt Acknowledgments 📢🤗\n",
    "A world of gratitude to the Optiver team for supplying the dataset and hosting an inspiring competition. This journey enriches us not just through the destination it leads to but also through the community and collective wisdom we amass.\n",
    "\n",
    "Set forth on this enhanced predictive expedition, and don't hesitate to connect for discussions or support!\n",
    "👉 [Explore my Kaggle projects](https://www.kaggle.com/verracodeguacas) 👈\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5acab6ce",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-11-05T12:19:09.338693Z",
     "iopub.status.busy": "2023-11-05T12:19:09.338314Z",
     "iopub.status.idle": "2023-11-05T12:19:14.445584Z",
     "shell.execute_reply": "2023-11-05T12:19:14.444776Z"
    },
    "papermill": {
     "duration": 5.118103,
     "end_time": "2023-11-05T12:19:14.447911",
     "exception": false,
     "start_time": "2023-11-05T12:19:09.329808",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import gc  # Garbage collection for memory management\n",
    "import os  # Operating system-related functions\n",
    "import time  # Time-related functions\n",
    "import warnings  # Handling warnings\n",
    "from itertools import combinations  # For creating combinations of elements\n",
    "from warnings import simplefilter  # Simplifying warning handling\n",
    "\n",
    "# 📦 Importing machine learning libraries\n",
    "import joblib  # For saving and loading models\n",
    "import lightgbm as lgb  # LightGBM gradient boosting framework\n",
    "import numpy as np  # Numerical operations\n",
    "import pandas as pd  # Data manipulation and analysis\n",
    "from sklearn.metrics import mean_absolute_error  # Metric for evaluation\n",
    "from sklearn.model_selection import KFold, TimeSeriesSplit  # Cross-validation techniques\n",
    "\n",
    "# 🤐 Disable warnings to keep the code clean\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "simplefilter(action=\"ignore\", category=pd.errors.PerformanceWarning)\n",
    "\n",
    "# 📊 Define flags and variables\n",
    "is_offline = False  # Flag for online/offline mode\n",
    "is_train = True  # Flag for training mode\n",
    "is_infer = True  # Flag for inference mode\n",
    "max_lookback = np.nan  # Maximum lookback (not specified)\n",
    "split_day = 435  # Split day for time series data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51915d6c",
   "metadata": {
    "papermill": {
     "duration": 0.007122,
     "end_time": "2023-11-05T12:19:14.462224",
     "exception": false,
     "start_time": "2023-11-05T12:19:14.455102",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 📊 Data Loading and Preprocessing 📊\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b66ad1",
   "metadata": {
    "papermill": {
     "duration": 0.006696,
     "end_time": "2023-11-05T12:19:14.475974",
     "exception": false,
     "start_time": "2023-11-05T12:19:14.469278",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Explaination**\n",
    "\n",
    "1. `df = pd.read_csv(\"/kaggle/input/optiver-trading-at-the-close/train.csv\")`\n",
    "   - This line reads a CSV (Comma-Separated Values) file named \"train.csv\" using the Pandas library and assigns the resulting DataFrame to the variable `df`. The CSV file is expected to be located at the specified file path, \"/kaggle/input/optiver-trading-at-the-close/train.csv\". This line is loading a dataset from a file.\n",
    "\n",
    "2. `df = df.dropna(subset=[\"target\"])`\n",
    "   - This line drops (removes) rows from the DataFrame `df` where there are missing values (NaN) in the \"target\" column. It uses the `dropna` method with the `subset` parameter set to \"target\" to specify that it should check for missing values in the \"target\" column and remove rows that have missing values. The updated DataFrame is assigned back to the variable `df`.\n",
    "\n",
    "3. `df.reset_index(drop=True, inplace=True)`\n",
    "   - This line resets the index of the DataFrame `df`. When data is removed from a DataFrame, the index labels may have gaps or may not be sequential. This line resets the index to be sequential, starting from 0, and the old index is dropped. The `drop=True` parameter indicates that the old index should be dropped, and `inplace=True` means that this operation modifies the DataFrame in place.\n",
    "\n",
    "4. `df_shape = df.shape`\n",
    "   - This line calculates the shape of the DataFrame `df`, which means it returns a tuple containing the number of rows and columns in the DataFrame. The result is assigned to the variable `df_shape`.\n",
    "\n",
    "To summarize, the code reads a dataset from a CSV file, removes rows with missing values in a specific column (\"target\"), resets the index of the DataFrame to make it sequential, and finally, it calculates and stores the shape of the resulting DataFrame in the `df_shape` variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc435c38",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-05T12:19:14.491871Z",
     "iopub.status.busy": "2023-11-05T12:19:14.490992Z",
     "iopub.status.idle": "2023-11-05T12:19:32.720863Z",
     "shell.execute_reply": "2023-11-05T12:19:32.719784Z"
    },
    "papermill": {
     "duration": 18.240502,
     "end_time": "2023-11-05T12:19:32.723280",
     "exception": false,
     "start_time": "2023-11-05T12:19:14.482778",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 📂 Read the dataset from a CSV file using Pandas\n",
    "df = pd.read_csv(\"/kaggle/input/optiver-trading-at-the-close/train.csv\")\n",
    "\n",
    "# 🧹 Remove rows with missing values in the \"target\" column\n",
    "df = df.dropna(subset=[\"target\"])\n",
    "\n",
    "# 🔁 Reset the index of the DataFrame and apply the changes in place\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# 📏 Get the shape of the DataFrame (number of rows and columns)\n",
    "df_shape = df.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5e700c",
   "metadata": {
    "papermill": {
     "duration": 0.006627,
     "end_time": "2023-11-05T12:19:32.737143",
     "exception": false,
     "start_time": "2023-11-05T12:19:32.730516",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 🚀 Memory Optimization Function with Data Type Conversion 🧹"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a96e5eb",
   "metadata": {
    "papermill": {
     "duration": 0.006855,
     "end_time": "2023-11-05T12:19:32.750768",
     "exception": false,
     "start_time": "2023-11-05T12:19:32.743913",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Explaination**\n",
    "\n",
    "This code defines a function `reduce_mem_usage` that is used to reduce the memory usage of a Pandas DataFrame by optimizing the data types of its columns. \n",
    "\n",
    "1. `def reduce_mem_usage(df, verbose=0):`\n",
    "   - This line defines a function called `reduce_mem_usage` that takes two parameters: `df`, which is the input Pandas DataFrame that needs memory optimization, and `verbose` (defaulting to 0), which is a flag to control whether or not to provide memory optimization information.\n",
    "\n",
    "2. `start_mem = df.memory_usage().sum() / 1024**2`\n",
    "   - This line calculates the initial memory usage of the input DataFrame `df` and stores it in the `start_mem` variable. It does this by using the `memory_usage()` method, which returns the memory usage of each column, and then sums these values. The result is divided by 1024^2 to convert it to megabytes.\n",
    "\n",
    "3. The code then enters a loop that iterates through each column of the DataFrame using the `for col in df.columns:` loop.\n",
    "\n",
    "4. Inside the loop, it checks the data type of the column using `col_type = df[col].dtype`.\n",
    "\n",
    "5. If the column's data type is not 'object' (i.e., it's numeric), it proceeds with the optimization.\n",
    "\n",
    "6. For integer columns:\n",
    "   - It checks the minimum and maximum values in the column (c_min and c_max).\n",
    "   - Depending on the range of values, it converts the column to the smallest integer data type that can accommodate the data while reducing memory usage. It checks for `int8`, `int16`, `int32`, and `int64` data types based on the data range.\n",
    "\n",
    "7. For float columns:\n",
    "   - Similar to integer columns, it checks the minimum and maximum values.\n",
    "   - It converts the column to a `float32` data type if the range is within the limits of `np.finfo(np.float32)`. The `np.finfo()` function is used to get the floating-point type's limits.\n",
    "\n",
    "8. If the column's data type is neither integer nor float and falls outside the specified ranges, it defaults to `float32`.\n",
    "\n",
    "9. If `verbose` is set to a truthy value (e.g., 1), it provides information about memory optimization, including the initial and final memory usage, and the percentage reduction in memory usage.\n",
    "\n",
    "10. Finally, the function returns the DataFrame with optimized memory usage.\n",
    "\n",
    "This function is useful for reducing the memory footprint of a DataFrame, especially when working with large datasets, by converting columns to the most memory-efficient data types based on the data they contain. It can help improve performance and reduce memory-related issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "370237a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-05T12:19:32.766316Z",
     "iopub.status.busy": "2023-11-05T12:19:32.765590Z",
     "iopub.status.idle": "2023-11-05T12:19:32.894464Z",
     "shell.execute_reply": "2023-11-05T12:19:32.893654Z"
    },
    "papermill": {
     "duration": 0.139016,
     "end_time": "2023-11-05T12:19:32.896481",
     "exception": false,
     "start_time": "2023-11-05T12:19:32.757465",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 🧹 Function to reduce memory usage of a Pandas DataFrame\n",
    "def reduce_mem_usage(df, verbose=0):\n",
    "    \"\"\"\n",
    "    Iterate through all numeric columns of a dataframe and modify the data type\n",
    "    to reduce memory usage.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 📏 Calculate the initial memory usage of the DataFrame\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "\n",
    "    # 🔄 Iterate through each column in the DataFrame\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "\n",
    "        # Check if the column's data type is not 'object' (i.e., numeric)\n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            \n",
    "            # Check if the column's data type is an integer\n",
    "            if str(col_type)[:3] == \"int\":\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                # Check if the column's data type is a float\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "\n",
    "    # ℹ️ Provide memory optimization information if 'verbose' is True\n",
    "    if verbose:\n",
    "        logger.info(f\"Memory usage of dataframe is {start_mem:.2f} MB\")\n",
    "        end_mem = df.memory_usage().sum() / 1024**2\n",
    "        logger.info(f\"Memory usage after optimization is: {end_mem:.2f} MB\")\n",
    "        decrease = 100 * (start_mem - end_mem) / start_mem\n",
    "        logger.info(f\"Decreased by {decrease:.2f}%\")\n",
    "\n",
    "    # 🔄 Return the DataFrame with optimized memory usage\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca228cb",
   "metadata": {
    "papermill": {
     "duration": 0.006686,
     "end_time": "2023-11-05T12:19:32.910838",
     "exception": false,
     "start_time": "2023-11-05T12:19:32.904152",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    " ## 🏎️Parallel Triplet Imbalance Calculation with Numba"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271755d8",
   "metadata": {
    "papermill": {
     "duration": 0.006517,
     "end_time": "2023-11-05T12:19:32.924104",
     "exception": false,
     "start_time": "2023-11-05T12:19:32.917587",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Explaination**\n",
    "\n",
    "\n",
    "This code includes functions for calculating triplet imbalance in a parallel and optimized manner using the Numba library. Let's break down each part of the code:\n",
    "\n",
    "1. `from numba import njit, prange`\n",
    "   - This line imports two important features from the Numba library: `njit` for Just-In-Time (JIT) compilation and `prange` for parallel processing. JIT compilation can significantly speed up the execution of code, and parallel processing allows for concurrent execution of code in a loop.\n",
    "\n",
    "2. `@njit(parallel=True)`\n",
    "   - This is a decorator applied to the `compute_triplet_imbalance` function, indicating that Numba should compile this function for speed optimization and parallel execution. This decorator makes use of Numba's features to enhance the performance of the code.\n",
    "\n",
    "3. `def compute_triplet_imbalance(df_values, comb_indices):`\n",
    "   - This function is designed to calculate triplet imbalance in a parallelized manner using Numba. It takes two parameters:\n",
    "     - `df_values`: A NumPy array containing the values of the DataFrame. It represents the price data.\n",
    "     - `comb_indices`: A list of combinations of three price indices (a, b, c) for which triplet imbalance needs to be computed.\n",
    "\n",
    "4. `num_rows = df_values.shape[0]`\n",
    "   - This line calculates the number of rows in the `df_values` array, which represents the number of rows in the DataFrame.\n",
    "\n",
    "5. `imbalance_features = np.empty((num_rows, num_combinations))`\n",
    "   - This line initializes an empty NumPy array `imbalance_features` with dimensions (number of rows, number of combinations). This array will store the computed triplet imbalance values.\n",
    "\n",
    "6. The code then enters a loop that iterates through all combinations of triplets specified by `comb_indices`.\n",
    "\n",
    "7. `for i in prange(num_combinations):`\n",
    "   - This loop is parallelized using `prange`, which allows for multiple combinations to be processed concurrently.\n",
    "\n",
    "8. Inside the loop, it extracts the indices (a, b, c) for the current combination.\n",
    "\n",
    "9. Another loop iterates through the rows of the DataFrame (`for j in range(num_rows)`) and calculates the triplet imbalance for each row.\n",
    "\n",
    "10. `max_val`, `min_val`, and `mid_val` are computed for each row.\n",
    "\n",
    "11. `if mid_val == min_val:` checks if division by zero would occur and sets the corresponding entry in `imbalance_features` to `np.nan` to prevent errors in such cases.\n",
    "\n",
    "12. The final imbalance value is calculated using the formula `(max_val - mid_val) / (mid_val - min_val)` and stored in the `imbalance_features` array.\n",
    "\n",
    "13. The function returns the `imbalance_features` array, which contains the computed triplet imbalance values for all combinations and rows.\n",
    "\n",
    "14. `calculate_triplet_imbalance_numba` is another function that takes a price column name and a DataFrame as input. It prepares the data and calculates triplet imbalance using the `compute_triplet_imbalance` function. It returns the result as a DataFrame with appropriately labeled columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "647397ec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-05T12:19:32.939420Z",
     "iopub.status.busy": "2023-11-05T12:19:32.939090Z",
     "iopub.status.idle": "2023-11-05T12:19:33.595330Z",
     "shell.execute_reply": "2023-11-05T12:19:33.594556Z"
    },
    "papermill": {
     "duration": 0.666737,
     "end_time": "2023-11-05T12:19:33.597740",
     "exception": false,
     "start_time": "2023-11-05T12:19:32.931003",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 🏎️ Import Numba for just-in-time (JIT) compilation and parallel processing\n",
    "from numba import njit, prange\n",
    "\n",
    "# 📊 Function to compute triplet imbalance in parallel using Numba\n",
    "@njit(parallel=True)\n",
    "def compute_triplet_imbalance(df_values, comb_indices):\n",
    "    num_rows = df_values.shape[0]\n",
    "    num_combinations = len(comb_indices)\n",
    "    imbalance_features = np.empty((num_rows, num_combinations))\n",
    "\n",
    "    # 🔁 Loop through all combinations of triplets\n",
    "    for i in prange(num_combinations):\n",
    "        a, b, c = comb_indices[i]\n",
    "        \n",
    "        # 🔁 Loop through rows of the DataFrame\n",
    "        for j in range(num_rows):\n",
    "            max_val = max(df_values[j, a], df_values[j, b], df_values[j, c])\n",
    "            min_val = min(df_values[j, a], df_values[j, b], df_values[j, c])\n",
    "            mid_val = df_values[j, a] + df_values[j, b] + df_values[j, c] - min_val - max_val\n",
    "            \n",
    "            # 🚫 Prevent division by zero\n",
    "            if mid_val == min_val:\n",
    "                imbalance_features[j, i] = np.nan\n",
    "            else:\n",
    "                imbalance_features[j, i] = (max_val - mid_val) / (mid_val - min_val)\n",
    "\n",
    "    return imbalance_features\n",
    "\n",
    "# 📈 Function to calculate triplet imbalance for given price data and a DataFrame\n",
    "def calculate_triplet_imbalance_numba(price, df):\n",
    "    # Convert DataFrame to numpy array for Numba compatibility\n",
    "    df_values = df[price].values\n",
    "    comb_indices = [(price.index(a), price.index(b), price.index(c)) for a, b, c in combinations(price, 3)]\n",
    "\n",
    "    # Calculate the triplet imbalance using the Numba-optimized function\n",
    "    features_array = compute_triplet_imbalance(df_values, comb_indices)\n",
    "\n",
    "    # Create a DataFrame from the results\n",
    "    columns = [f\"{a}_{b}_{c}_imb2\" for a, b, c in combinations(price, 3)]\n",
    "    features = pd.DataFrame(features_array, columns=columns)\n",
    "\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77099bd9",
   "metadata": {
    "papermill": {
     "duration": 0.006707,
     "end_time": "2023-11-05T12:19:33.611671",
     "exception": false,
     "start_time": "2023-11-05T12:19:33.604964",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 📊 Feature Generation Functions 📊\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd30d5fa",
   "metadata": {
    "papermill": {
     "duration": 0.006519,
     "end_time": "2023-11-05T12:19:33.625104",
     "exception": false,
     "start_time": "2023-11-05T12:19:33.618585",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Explaination**\n",
    "\n",
    "\n",
    "\n",
    "1. `imbalance_features(df)`:\n",
    "   - This function takes a DataFrame `df` as input.\n",
    "   - It calculates various features related to price and size data using Pandas' `eval` function, creating new columns in the DataFrame for each feature.\n",
    "   - It then creates pairwise price imbalance features for combinations of price columns.\n",
    "   - Next, it calculates triplet imbalance features using the Numba-optimized function `calculate_triplet_imbalance_numba`.\n",
    "   - Finally, it calculates additional features, including momentum, spread, intensity, pressure, market urgency, and depth pressure.\n",
    "   - It also calculates statistical aggregation features (mean, standard deviation, skewness, kurtosis) for both price and size columns.\n",
    "   - Shifted, return, and diff features are generated for specific columns.\n",
    "   - Infinite values in the DataFrame are replaced with 0.\n",
    "\n",
    "2. `other_features(df)`:\n",
    "   - This function adds time-related and stock-related features to the DataFrame.\n",
    "   - It calculates the day of the week, seconds, and minutes from the \"date_id\" and \"seconds_in_bucket\" columns.\n",
    "   - It maps global features from a predefined dictionary to the DataFrame based on the \"stock_id.\"\n",
    "\n",
    "3. `generate_all_features(df)`:\n",
    "   - This function combines the features generated by the `imbalance_features` and `other_features` functions.\n",
    "   - It selects the relevant columns for feature generation, applies the `imbalance_features` function, adds time and stock-related features using the `other_features` function, and then performs garbage collection to free up memory.\n",
    "   - The function returns a DataFrame containing the generated features, excluding certain columns like \"row_id,\" \"target,\" \"time_id,\" and \"date_id.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c94b775",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-05T12:19:33.640492Z",
     "iopub.status.busy": "2023-11-05T12:19:33.639848Z",
     "iopub.status.idle": "2023-11-05T12:19:33.658804Z",
     "shell.execute_reply": "2023-11-05T12:19:33.657866Z"
    },
    "papermill": {
     "duration": 0.029023,
     "end_time": "2023-11-05T12:19:33.660873",
     "exception": false,
     "start_time": "2023-11-05T12:19:33.631850",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 📊 Function to generate imbalance features\n",
    "def imbalance_features(df):\n",
    "    # Define lists of price and size-related column names\n",
    "    prices = [\"reference_price\", \"far_price\", \"near_price\", \"ask_price\", \"bid_price\", \"wap\"]\n",
    "    sizes = [\"matched_size\", \"bid_size\", \"ask_size\", \"imbalance_size\"]\n",
    "\n",
    "    # V1 features\n",
    "    # Calculate various features using Pandas eval function\n",
    "    df[\"volume\"] = df.eval(\"ask_size + bid_size\")\n",
    "    df[\"mid_price\"] = df.eval(\"(ask_price + bid_price) / 2\")\n",
    "    df[\"liquidity_imbalance\"] = df.eval(\"(bid_size-ask_size)/(bid_size+ask_size)\")\n",
    "    df[\"matched_imbalance\"] = df.eval(\"(imbalance_size-matched_size)/(matched_size+imbalance_size)\")\n",
    "    df[\"size_imbalance\"] = df.eval(\"bid_size / ask_size\")\n",
    "    \n",
    "    # Create features for pairwise price imbalances\n",
    "    for c in combinations(prices, 2):\n",
    "        df[f\"{c[0]}_{c[1]}_imb\"] = df.eval(f\"({c[0]} - {c[1]})/({c[0]} + {c[1]})\")\n",
    "\n",
    "    # Calculate triplet imbalance features using the Numba-optimized function\n",
    "    for c in [['ask_price', 'bid_price', 'wap', 'reference_price'], sizes]:\n",
    "        triplet_feature = calculate_triplet_imbalance_numba(c, df)\n",
    "        df[triplet_feature.columns] = triplet_feature.values\n",
    "        \n",
    "    # V2 features\n",
    "    # Calculate additional features\n",
    "    df[\"imbalance_momentum\"] = df.groupby(['stock_id'])['imbalance_size'].diff(periods=1) / df['matched_size']\n",
    "    df[\"price_spread\"] = df[\"ask_price\"] - df[\"bid_price\"]\n",
    "    df[\"spread_intensity\"] = df.groupby(['stock_id'])['price_spread'].diff()\n",
    "    df['price_pressure'] = df['imbalance_size'] * (df['ask_price'] - df['bid_price'])\n",
    "    df['market_urgency'] = df['price_spread'] * df['liquidity_imbalance']\n",
    "    df['depth_pressure'] = (df['ask_size'] - df['bid_size']) * (df['far_price'] - df['near_price'])\n",
    "    \n",
    "    # Calculate various statistical aggregation features\n",
    "    for func in [\"mean\", \"std\", \"skew\", \"kurt\"]:\n",
    "        df[f\"all_prices_{func}\"] = df[prices].agg(func, axis=1)\n",
    "        df[f\"all_sizes_{func}\"] = df[sizes].agg(func, axis=1)\n",
    "        \n",
    "    # V3 features\n",
    "    # Calculate shifted and return features for specific columns\n",
    "    for col in ['matched_size', 'imbalance_size', 'reference_price', 'imbalance_buy_sell_flag']:\n",
    "        for window in [1, 2, 3, 10]:\n",
    "            df[f\"{col}_shift_{window}\"] = df.groupby('stock_id')[col].shift(window)\n",
    "            df[f\"{col}_ret_{window}\"] = df.groupby('stock_id')[col].pct_change(window)\n",
    "    \n",
    "    # Calculate diff features for specific columns\n",
    "    for col in ['ask_price', 'bid_price', 'ask_size', 'bid_size']:\n",
    "        for window in [1, 2, 3, 10]:\n",
    "            df[f\"{col}_diff_{window}\"] = df.groupby(\"stock_id\")[col].diff(window)\n",
    "\n",
    "    # Replace infinite values with 0\n",
    "    return df.replace([np.inf, -np.inf], 0)\n",
    "\n",
    "# 📅 Function to generate time and stock-related features\n",
    "def other_features(df):\n",
    "    df[\"dow\"] = df[\"date_id\"] % 5  # Day of the week\n",
    "    df[\"seconds\"] = df[\"seconds_in_bucket\"] % 60  # Seconds\n",
    "    df[\"minute\"] = df[\"seconds_in_bucket\"] // 60  # Minutes\n",
    "\n",
    "    # Map global features to the DataFrame\n",
    "    for key, value in global_stock_id_feats.items():\n",
    "        df[f\"global_{key}\"] = df[\"stock_id\"].map(value.to_dict())\n",
    "\n",
    "    return df\n",
    "\n",
    "# 🚀 Function to generate all features by combining imbalance and other features\n",
    "def generate_all_features(df):\n",
    "    # Select relevant columns for feature generation\n",
    "    cols = [c for c in df.columns if c not in [\"row_id\", \"time_id\", \"target\"]]\n",
    "    df = df[cols]\n",
    "    \n",
    "    # Generate imbalance features\n",
    "    df = imbalance_features(df)\n",
    "    \n",
    "    # Generate time and stock-related features\n",
    "    df = other_features(df)\n",
    "    gc.collect()  # Perform garbage collection to free up memory\n",
    "    \n",
    "    # Select and return the generated features\n",
    "    feature_name = [i for i in df.columns if i not in [\"row_id\", \"target\", \"time_id\", \"date_id\"]]\n",
    "    \n",
    "    return df[feature_name]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed278ab3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-05T12:19:33.676942Z",
     "iopub.status.busy": "2023-11-05T12:19:33.676315Z",
     "iopub.status.idle": "2023-11-05T12:19:33.689015Z",
     "shell.execute_reply": "2023-11-05T12:19:33.688090Z"
    },
    "papermill": {
     "duration": 0.022979,
     "end_time": "2023-11-05T12:19:33.690928",
     "exception": false,
     "start_time": "2023-11-05T12:19:33.667949",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "weights = [\n",
    "    0.004, 0.001, 0.002, 0.006, 0.004, 0.004, 0.002, 0.006, 0.006, 0.002, 0.002, 0.008,\n",
    "    0.006, 0.002, 0.008, 0.006, 0.002, 0.006, 0.004, 0.002, 0.004, 0.001, 0.006, 0.004,\n",
    "    0.002, 0.002, 0.004, 0.002, 0.004, 0.004, 0.001, 0.001, 0.002, 0.002, 0.006, 0.004,\n",
    "    0.004, 0.004, 0.006, 0.002, 0.002, 0.04 , 0.002, 0.002, 0.004, 0.04 , 0.002, 0.001,\n",
    "    0.006, 0.004, 0.004, 0.006, 0.001, 0.004, 0.004, 0.002, 0.006, 0.004, 0.006, 0.004,\n",
    "    0.006, 0.004, 0.002, 0.001, 0.002, 0.004, 0.002, 0.008, 0.004, 0.004, 0.002, 0.004,\n",
    "    0.006, 0.002, 0.004, 0.004, 0.002, 0.004, 0.004, 0.004, 0.001, 0.002, 0.002, 0.008,\n",
    "    0.02 , 0.004, 0.006, 0.002, 0.02 , 0.002, 0.002, 0.006, 0.004, 0.002, 0.001, 0.02,\n",
    "    0.006, 0.001, 0.002, 0.004, 0.001, 0.002, 0.006, 0.006, 0.004, 0.006, 0.001, 0.002,\n",
    "    0.004, 0.006, 0.006, 0.001, 0.04 , 0.006, 0.002, 0.004, 0.002, 0.002, 0.006, 0.002,\n",
    "    0.002, 0.004, 0.006, 0.006, 0.002, 0.002, 0.008, 0.006, 0.004, 0.002, 0.006, 0.002,\n",
    "    0.004, 0.006, 0.002, 0.004, 0.001, 0.004, 0.002, 0.004, 0.008, 0.006, 0.008, 0.002,\n",
    "    0.004, 0.002, 0.001, 0.004, 0.004, 0.004, 0.006, 0.008, 0.004, 0.001, 0.001, 0.002,\n",
    "    0.006, 0.004, 0.001, 0.002, 0.006, 0.004, 0.006, 0.008, 0.002, 0.002, 0.004, 0.002,\n",
    "    0.04 , 0.002, 0.002, 0.004, 0.002, 0.002, 0.006, 0.02 , 0.004, 0.002, 0.006, 0.02,\n",
    "    0.001, 0.002, 0.006, 0.004, 0.006, 0.004, 0.004, 0.004, 0.004, 0.002, 0.004, 0.04,\n",
    "    0.002, 0.008, 0.002, 0.004, 0.001, 0.004, 0.006, 0.004,\n",
    "]\n",
    "\n",
    "weights = {int(k):v for k,v in enumerate(weights)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80536af8",
   "metadata": {
    "papermill": {
     "duration": 0.006675,
     "end_time": "2023-11-05T12:19:33.704872",
     "exception": false,
     "start_time": "2023-11-05T12:19:33.698197",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Data Splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45946db",
   "metadata": {
    "papermill": {
     "duration": 0.006568,
     "end_time": "2023-11-05T12:19:33.718239",
     "exception": false,
     "start_time": "2023-11-05T12:19:33.711671",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Explaination**\n",
    "\n",
    "Checks whether it is running in offline or online mode and takes different actions accordingly. Here's what each part of the code does:\n",
    "\n",
    "1. `if is_offline:`:\n",
    "   - This condition checks if the variable `is_offline` is `True`. If it is `True`, it means the code is running in offline mode. \n",
    "\n",
    "2. In the offline mode block:\n",
    "   - The code splits the dataset into two parts: `df_train` and `df_valid` based on the value of the `split_day`. Data with \"date_id\" less than or equal to the `split_day` is assigned to `df_train`, while data with \"date_id\" greater than the `split_day` is assigned to `df_valid`.\n",
    "   - It then displays a message indicating that the code is running in offline mode and provides the shapes (number of rows and columns) of the training and validation sets using the `print` statements.\n",
    "\n",
    "3. In the online mode block:\n",
    "   - If the code is not running in offline mode (i.e., `is_offline` is `False`), it means it's running in online mode.\n",
    "   - In online mode, the entire dataset is used for training, and the entire dataset is assigned to `df_train`.\n",
    "   - It displays a message indicating that the code is running in online mode using the `print` statement.\n",
    "\n",
    "The purpose of distinguishing between offline and online modes is often related to the context in which the code is used. In offline mode, you typically have historical data and can perform tasks like data splitting for training and validation, while in online mode, you might be working with real-time data and use the entire dataset for training. The choice of mode can impact the preprocessing and analysis steps that follow in the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e51a5a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-05T12:19:33.733930Z",
     "iopub.status.busy": "2023-11-05T12:19:33.733475Z",
     "iopub.status.idle": "2023-11-05T12:19:33.739627Z",
     "shell.execute_reply": "2023-11-05T12:19:33.738737Z"
    },
    "papermill": {
     "duration": 0.016167,
     "end_time": "2023-11-05T12:19:33.741631",
     "exception": false,
     "start_time": "2023-11-05T12:19:33.725464",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Online mode\n"
     ]
    }
   ],
   "source": [
    "# Check if the code is running in offline or online mode\n",
    "if is_offline:\n",
    "    # In offline mode, split the data into training and validation sets based on the split_day\n",
    "    df_train = df[df[\"date_id\"] <= split_day]\n",
    "    df_valid = df[df[\"date_id\"] > split_day]\n",
    "    \n",
    "    # Display a message indicating offline mode and the shapes of the training and validation sets\n",
    "    print(\"Offline mode\")\n",
    "    print(f\"train : {df_train.shape}, valid : {df_valid.shape}\")\n",
    "else:\n",
    "    # In online mode, use the entire dataset for training\n",
    "    df_train = df\n",
    "    \n",
    "    # Display a message indicating online mode\n",
    "    print(\"Online mode\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c314b9",
   "metadata": {
    "papermill": {
     "duration": 0.006613,
     "end_time": "2023-11-05T12:19:33.754999",
     "exception": false,
     "start_time": "2023-11-05T12:19:33.748386",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Explaination**\n",
    "\n",
    "\n",
    "\n",
    "1. `if is_train:`\n",
    "   - This condition checks if the variable `is_train` is `True`. If it is `True`, it means that the code is being executed in a training context.\n",
    "\n",
    "2. Inside the `if is_train:` block:\n",
    "   - A dictionary named `global_stock_id_feats` is created. This dictionary contains various statistical summary features calculated for each stock_id. These features include the median, standard deviation, and range of bid sizes and ask sizes, as well as bid prices and ask prices. These statistics are computed based on the training data (`df_train`) using Pandas' `groupby` and aggregation functions.\n",
    "\n",
    "3. The code checks if the execution mode is offline (`is_offline`) by further nested conditions.\n",
    "   - If it is offline (`is_offline` is `True`):\n",
    "     - It generates features for the training set (`df_train`) using the `generate_all_features` function.\n",
    "     - It prints a message indicating that the process of building the training features is finished.\n",
    "     - It generates features for the validation set (`df_valid`) using the `generate_all_features` function.\n",
    "     - It prints a message indicating that the process of building the validation features is finished.\n",
    "     - It reduces memory usage of the validation features using the `reduce_mem_usage` function.\n",
    "\n",
    "   - If it is not in offline mode (i.e., online mode):\n",
    "     - It generates features for the training set (`df_train`) using the `generate_all_features` function.\n",
    "     - It prints a message indicating that the process of building online training features is finished.\n",
    "\n",
    "4. After generating features, it reduces memory usage of the training features (`df_train_feats`) using the `reduce_mem_usage` function. This is done to optimize memory consumption and improve performance.\n",
    "\n",
    "The code's purpose is to prepare and optimize the feature set for training, considering whether it is in offline or online mode and whether it's part of the training process. The generated features and memory optimization are important steps in machine learning workflows, as they impact the training process and the model's efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "898508c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-05T12:19:33.769873Z",
     "iopub.status.busy": "2023-11-05T12:19:33.769521Z",
     "iopub.status.idle": "2023-11-05T12:20:32.807611Z",
     "shell.execute_reply": "2023-11-05T12:20:32.806745Z"
    },
    "papermill": {
     "duration": 59.048473,
     "end_time": "2023-11-05T12:20:32.810127",
     "exception": false,
     "start_time": "2023-11-05T12:19:33.761654",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build Online Train Feats Finished.\n"
     ]
    }
   ],
   "source": [
    "if is_train:\n",
    "    global_stock_id_feats = {\n",
    "        \"median_size\": df_train.groupby(\"stock_id\")[\"bid_size\"].median() + df_train.groupby(\"stock_id\")[\"ask_size\"].median(),\n",
    "        \"std_size\": df_train.groupby(\"stock_id\")[\"bid_size\"].std() + df_train.groupby(\"stock_id\")[\"ask_size\"].std(),\n",
    "        \"ptp_size\": df_train.groupby(\"stock_id\")[\"bid_size\"].max() - df_train.groupby(\"stock_id\")[\"bid_size\"].min(),\n",
    "        \"median_price\": df_train.groupby(\"stock_id\")[\"bid_price\"].median() + df_train.groupby(\"stock_id\")[\"ask_price\"].median(),\n",
    "        \"std_price\": df_train.groupby(\"stock_id\")[\"bid_price\"].std() + df_train.groupby(\"stock_id\")[\"ask_price\"].std(),\n",
    "        \"ptp_price\": df_train.groupby(\"stock_id\")[\"bid_price\"].max() - df_train.groupby(\"stock_id\")[\"ask_price\"].min(),\n",
    "    }\n",
    "    if is_offline:\n",
    "        df_train_feats = generate_all_features(df_train)\n",
    "        print(\"Build Train Feats Finished.\")\n",
    "        df_valid_feats = generate_all_features(df_valid)\n",
    "        print(\"Build Valid Feats Finished.\")\n",
    "        df_valid_feats = reduce_mem_usage(df_valid_feats)\n",
    "    else:\n",
    "        df_train_feats = generate_all_features(df_train)\n",
    "        print(\"Build Online Train Feats Finished.\")\n",
    "\n",
    "    df_train_feats = reduce_mem_usage(df_train_feats)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c2b330",
   "metadata": {
    "papermill": {
     "duration": 0.006838,
     "end_time": "2023-11-05T12:20:32.824112",
     "exception": false,
     "start_time": "2023-11-05T12:20:32.817274",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "## Cross-Validation Strategy 📊\n",
    "\n",
    "In our model development, we have adopted a somewhat unconventional approach to time-series cross-validation known as \"purged k-fold validation.\" This technique is tailored for financial time series data, where the traditional k-fold strategy might lead to look-ahead bias due to the autocorrelation inherent in such data.\n",
    "\n",
    "### Purged K-Fold Validation 🚫\n",
    "\n",
    "The standard practice in time-series analysis is to use rolling or expanding window cross-validation. However, our strategy is to divide the entire dataset into five distinct folds based on `date_id`, which spans 480 days in total.\n",
    "\n",
    "We purposefully introduce a \"purge\" period—a gap between the training and validation sets. This purging process is meant to prevent the leakage of information from the validation set back into the training set. For each fold, we train on data occurring before this purge period and validate on data following it.\n",
    "\n",
    "### Five-Fold Expansion 🖐️\n",
    "\n",
    "Expanding to five folds from a smaller number, as we have chosen to do, is different from standard practice. Typically, time-series validation is done with forward chaining, where newer data is used for validation. In contrast, we are using past data for validation 🔄. We've taken this route under the assumption that market microstructure does not undergo drastic changes over a 480-day window, thus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f1654f00",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-05T12:20:32.840050Z",
     "iopub.status.busy": "2023-11-05T12:20:32.839231Z",
     "iopub.status.idle": "2023-11-05T13:49:28.305499Z",
     "shell.execute_reply": "2023-11-05T13:49:28.304484Z"
    },
    "papermill": {
     "duration": 5335.494275,
     "end_time": "2023-11-05T13:49:28.325328",
     "exception": false,
     "start_time": "2023-11-05T12:20:32.831053",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature length = 112\n",
      "Fold 1 Model Training\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's l1: 5.60873\n",
      "[200]\tvalid_0's l1: 5.57853\n",
      "[300]\tvalid_0's l1: 5.56609\n",
      "[400]\tvalid_0's l1: 5.55786\n",
      "[500]\tvalid_0's l1: 5.55279\n",
      "[600]\tvalid_0's l1: 5.54935\n",
      "[700]\tvalid_0's l1: 5.54694\n",
      "[800]\tvalid_0's l1: 5.54533\n",
      "[900]\tvalid_0's l1: 5.54438\n",
      "[1000]\tvalid_0's l1: 5.54342\n",
      "[1100]\tvalid_0's l1: 5.54303\n",
      "[1200]\tvalid_0's l1: 5.54254\n",
      "[1300]\tvalid_0's l1: 5.54238\n",
      "[1400]\tvalid_0's l1: 5.54213\n",
      "[1500]\tvalid_0's l1: 5.54187\n",
      "[1600]\tvalid_0's l1: 5.54171\n",
      "[1700]\tvalid_0's l1: 5.54165\n",
      "[1800]\tvalid_0's l1: 5.54152\n",
      "[1900]\tvalid_0's l1: 5.54149\n",
      "[2000]\tvalid_0's l1: 5.54137\n",
      "[2100]\tvalid_0's l1: 5.54127\n",
      "[2200]\tvalid_0's l1: 5.54128\n",
      "Early stopping, best iteration is:\n",
      "[2110]\tvalid_0's l1: 5.54126\n",
      "Model for fold 1 saved to modelitos_para_despues/doblez_1.txt\n",
      "Fold 1 MAE: 5.541261078364108\n",
      "Fold 2 Model Training\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's l1: 7.01331\n",
      "[200]\tvalid_0's l1: 6.97499\n",
      "[300]\tvalid_0's l1: 6.95843\n",
      "[400]\tvalid_0's l1: 6.94813\n",
      "[500]\tvalid_0's l1: 6.94093\n",
      "[600]\tvalid_0's l1: 6.93613\n",
      "[700]\tvalid_0's l1: 6.93267\n",
      "[800]\tvalid_0's l1: 6.93002\n",
      "[900]\tvalid_0's l1: 6.92844\n",
      "[1000]\tvalid_0's l1: 6.92742\n",
      "[1100]\tvalid_0's l1: 6.92675\n",
      "[1200]\tvalid_0's l1: 6.92607\n",
      "[1300]\tvalid_0's l1: 6.92542\n",
      "[1400]\tvalid_0's l1: 6.92508\n",
      "[1500]\tvalid_0's l1: 6.92465\n",
      "[1600]\tvalid_0's l1: 6.92443\n",
      "[1700]\tvalid_0's l1: 6.92407\n",
      "[1800]\tvalid_0's l1: 6.92366\n",
      "[1900]\tvalid_0's l1: 6.9231\n",
      "[2000]\tvalid_0's l1: 6.92276\n",
      "[2100]\tvalid_0's l1: 6.92257\n",
      "[2200]\tvalid_0's l1: 6.92222\n",
      "[2300]\tvalid_0's l1: 6.92196\n",
      "[2400]\tvalid_0's l1: 6.92169\n",
      "[2500]\tvalid_0's l1: 6.92157\n",
      "[2600]\tvalid_0's l1: 6.92147\n",
      "[2700]\tvalid_0's l1: 6.92137\n",
      "[2800]\tvalid_0's l1: 6.92128\n",
      "[2900]\tvalid_0's l1: 6.92111\n",
      "[3000]\tvalid_0's l1: 6.92099\n",
      "[3100]\tvalid_0's l1: 6.92087\n",
      "[3200]\tvalid_0's l1: 6.92075\n",
      "[3300]\tvalid_0's l1: 6.92066\n",
      "[3400]\tvalid_0's l1: 6.92047\n",
      "[3500]\tvalid_0's l1: 6.92034\n",
      "Early stopping, best iteration is:\n",
      "[3477]\tvalid_0's l1: 6.92032\n",
      "Model for fold 2 saved to modelitos_para_despues/doblez_2.txt\n",
      "Fold 2 MAE: 6.920322350849693\n",
      "Fold 3 Model Training\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's l1: 6.35839\n",
      "[200]\tvalid_0's l1: 6.33379\n",
      "[300]\tvalid_0's l1: 6.32312\n",
      "[400]\tvalid_0's l1: 6.31522\n",
      "[500]\tvalid_0's l1: 6.30981\n",
      "[600]\tvalid_0's l1: 6.30617\n",
      "[700]\tvalid_0's l1: 6.3037\n",
      "[800]\tvalid_0's l1: 6.30183\n",
      "[900]\tvalid_0's l1: 6.30055\n",
      "[1000]\tvalid_0's l1: 6.2997\n",
      "[1100]\tvalid_0's l1: 6.29897\n",
      "[1200]\tvalid_0's l1: 6.29837\n",
      "[1300]\tvalid_0's l1: 6.29807\n",
      "[1400]\tvalid_0's l1: 6.29778\n",
      "[1500]\tvalid_0's l1: 6.29761\n",
      "[1600]\tvalid_0's l1: 6.29733\n",
      "[1700]\tvalid_0's l1: 6.29715\n",
      "[1800]\tvalid_0's l1: 6.29682\n",
      "[1900]\tvalid_0's l1: 6.29658\n",
      "[2000]\tvalid_0's l1: 6.29647\n",
      "[2100]\tvalid_0's l1: 6.29633\n",
      "[2200]\tvalid_0's l1: 6.2963\n",
      "[2300]\tvalid_0's l1: 6.29627\n",
      "[2400]\tvalid_0's l1: 6.29624\n",
      "[2500]\tvalid_0's l1: 6.29625\n",
      "Early stopping, best iteration is:\n",
      "[2445]\tvalid_0's l1: 6.29623\n",
      "Model for fold 3 saved to modelitos_para_despues/doblez_3.txt\n",
      "Fold 3 MAE: 6.296229963553313\n",
      "Fold 4 Model Training\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's l1: 6.34859\n",
      "[200]\tvalid_0's l1: 6.32444\n",
      "[300]\tvalid_0's l1: 6.31406\n",
      "[400]\tvalid_0's l1: 6.30569\n",
      "[500]\tvalid_0's l1: 6.30015\n",
      "[600]\tvalid_0's l1: 6.29678\n",
      "[700]\tvalid_0's l1: 6.2948\n",
      "[800]\tvalid_0's l1: 6.29285\n",
      "[900]\tvalid_0's l1: 6.2917\n",
      "[1000]\tvalid_0's l1: 6.29115\n",
      "[1100]\tvalid_0's l1: 6.29066\n",
      "[1200]\tvalid_0's l1: 6.29042\n",
      "[1300]\tvalid_0's l1: 6.2903\n",
      "[1400]\tvalid_0's l1: 6.28995\n",
      "[1500]\tvalid_0's l1: 6.28968\n",
      "[1600]\tvalid_0's l1: 6.28944\n",
      "[1700]\tvalid_0's l1: 6.28934\n",
      "[1800]\tvalid_0's l1: 6.28926\n",
      "[1900]\tvalid_0's l1: 6.28927\n",
      "[2000]\tvalid_0's l1: 6.28925\n",
      "Early stopping, best iteration is:\n",
      "[1957]\tvalid_0's l1: 6.28924\n",
      "Model for fold 4 saved to modelitos_para_despues/doblez_4.txt\n",
      "Fold 4 MAE: 6.289239654170258\n",
      "Fold 5 Model Training\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's l1: 5.99604\n",
      "[200]\tvalid_0's l1: 5.97256\n",
      "[300]\tvalid_0's l1: 5.96286\n",
      "[400]\tvalid_0's l1: 5.95538\n",
      "[500]\tvalid_0's l1: 5.95046\n",
      "[600]\tvalid_0's l1: 5.94749\n",
      "[700]\tvalid_0's l1: 5.94585\n",
      "[800]\tvalid_0's l1: 5.9447\n",
      "[900]\tvalid_0's l1: 5.94395\n",
      "[1000]\tvalid_0's l1: 5.94353\n",
      "[1100]\tvalid_0's l1: 5.9432\n",
      "[1200]\tvalid_0's l1: 5.94308\n",
      "[1300]\tvalid_0's l1: 5.94274\n",
      "[1400]\tvalid_0's l1: 5.94266\n",
      "[1500]\tvalid_0's l1: 5.94252\n",
      "[1600]\tvalid_0's l1: 5.94238\n",
      "[1700]\tvalid_0's l1: 5.94229\n",
      "[1800]\tvalid_0's l1: 5.94216\n",
      "[1900]\tvalid_0's l1: 5.94209\n",
      "[2000]\tvalid_0's l1: 5.94212\n",
      "Early stopping, best iteration is:\n",
      "[1908]\tvalid_0's l1: 5.94207\n",
      "Model for fold 5 saved to modelitos_para_despues/doblez_5.txt\n",
      "Fold 5 MAE: 5.942070840603586\n",
      "Training final model with average best iteration: 2379\n",
      "Final model saved to modelitos_para_despues/doblez-conjunto.txt\n",
      "Average MAE across all folds: 6.197824777508191\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import gc\n",
    "\n",
    "# Assuming df_train_feats and df_train are already defined and df_train contains the 'date_id' column\n",
    "\n",
    "# Set up parameters for LightGBM\n",
    "\n",
    "lgb_params = {\n",
    "    \"objective\": \"mae\",\n",
    "    \"n_estimators\": 5000,\n",
    "    \"num_leaves\": 256,\n",
    "    \"subsample\": 0.6,\n",
    "    \"colsample_bytree\": 0.6,\n",
    "    \"learning_rate\": 0.00871,\n",
    "    \"n_jobs\": 4,\n",
    "    \"device\": \"gpu\",\n",
    "    \"verbosity\": -1,\n",
    "    \"importance_type\": \"gain\",\n",
    "}\n",
    "feature_name = list(df_train_feats.columns)\n",
    "print(f\"Feature length = {len(feature_name)}\")\n",
    "\n",
    "# The total number of date_ids is 480, we split them into 5 folds with a gap of 5 days in between\n",
    "num_folds = 5\n",
    "fold_size = 480 // num_folds\n",
    "gap = 5\n",
    "\n",
    "models = []\n",
    "scores = []\n",
    "\n",
    "model_save_path = 'modelitos_para_despues'  # Directory to save models\n",
    "if not os.path.exists(model_save_path):\n",
    "    os.makedirs(model_save_path)\n",
    "\n",
    "# We need to use the date_id from df_train to split the data\n",
    "date_ids = df_train['date_id'].values\n",
    "\n",
    "for i in range(num_folds):\n",
    "    start = i * fold_size\n",
    "    end = start + fold_size\n",
    "    \n",
    "    # Define the purged set ranges\n",
    "    purged_before_start = start - 2\n",
    "    purged_before_end = start + 2\n",
    "    purged_after_start = end - 2\n",
    "    purged_after_end = end + 2\n",
    "    \n",
    "    # Exclude the purged ranges from the test set\n",
    "    purged_set = ((date_ids >= purged_before_start) & (date_ids <= purged_before_end)) | \\\n",
    "                 ((date_ids >= purged_after_start) & (date_ids <= purged_after_end))\n",
    "    \n",
    "    # Define test_indices excluding the purged set\n",
    "    test_indices = (date_ids >= start) & (date_ids < end) & ~purged_set\n",
    "    train_indices = ~test_indices & ~purged_set\n",
    "    \n",
    "    df_fold_train = df_train_feats[train_indices]\n",
    "    df_fold_train_target = df_train['target'][train_indices]\n",
    "    df_fold_valid = df_train_feats[test_indices]\n",
    "    df_fold_valid_target = df_train['target'][test_indices]\n",
    "\n",
    "    print(f\"Fold {i+1} Model Training\")\n",
    "    \n",
    "    # Train a LightGBM model for the current fold\n",
    "    lgb_model = lgb.LGBMRegressor(**lgb_params)\n",
    "    lgb_model.fit(\n",
    "        df_fold_train[feature_name],\n",
    "        df_fold_train_target,\n",
    "        eval_set=[(df_fold_valid[feature_name], df_fold_valid_target)],\n",
    "        callbacks=[\n",
    "            lgb.callback.early_stopping(stopping_rounds=100),\n",
    "            lgb.callback.log_evaluation(period=100),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # Append the model to the list\n",
    "    models.append(lgb_model)\n",
    "    # Save the model to a file\n",
    "    model_filename = os.path.join(model_save_path, f'doblez_{i+1}.txt')\n",
    "    lgb_model.booster_.save_model(model_filename)\n",
    "    print(f\"Model for fold {i+1} saved to {model_filename}\")\n",
    "\n",
    "    # Evaluate model performance on the validation set\n",
    "    fold_predictions = lgb_model.predict(df_fold_valid[feature_name])\n",
    "    fold_score = mean_absolute_error(fold_predictions, df_fold_valid_target)\n",
    "    scores.append(fold_score)\n",
    "    print(f\"Fold {i+1} MAE: {fold_score}\")\n",
    "\n",
    "    # Free up memory by deleting fold specific variables\n",
    "    del df_fold_train, df_fold_train_target, df_fold_valid, df_fold_valid_target\n",
    "    gc.collect()\n",
    "\n",
    "# Calculate the average best iteration from all regular folds\n",
    "average_best_iteration = int(np.mean([model.best_iteration_ for model in models]))\n",
    "\n",
    "# Update the lgb_params with the average best iteration\n",
    "final_model_params = lgb_params.copy()\n",
    "final_model_params['n_estimators'] = average_best_iteration\n",
    "\n",
    "print(f\"Training final model with average best iteration: {average_best_iteration}\")\n",
    "\n",
    "# Train the final model on the entire dataset\n",
    "final_model = lgb.LGBMRegressor(**final_model_params)\n",
    "final_model.fit(\n",
    "    df_train_feats[feature_name],\n",
    "    df_train['target'],\n",
    "    callbacks=[\n",
    "        lgb.callback.log_evaluation(period=100),\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Append the final model to the list of models\n",
    "models.append(final_model)\n",
    "\n",
    "# Save the final model to a file\n",
    "final_model_filename = os.path.join(model_save_path, 'doblez-conjunto.txt')\n",
    "final_model.booster_.save_model(final_model_filename)\n",
    "print(f\"Final model saved to {final_model_filename}\")\n",
    "\n",
    "# Now 'models' holds the trained models for each fold and 'scores' holds the validation scores\n",
    "print(f\"Average MAE across all folds: {np.mean(scores)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8790ae9d",
   "metadata": {
    "papermill": {
     "duration": 0.01745,
     "end_time": "2023-11-05T13:49:28.360649",
     "exception": false,
     "start_time": "2023-11-05T13:49:28.343199",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Explaination**\n",
    "\n",
    "This code block is responsible for making predictions in inference mode and submitting them to the Optiver 2023 competition environment. \n",
    "\n",
    "1. `def zero_sum(prices, volumes):`\n",
    "   - This function takes two NumPy arrays, `prices` and `volumes`, as input.\n",
    "   - It calculates the standard error as the square root of the `volumes`.\n",
    "   - It calculates a variable `step` by dividing the sum of `prices` by the sum of the standard errors (`std_error`).\n",
    "   - It calculates the `out` variable as the difference between the `prices` and the product of the `std_error` and the `step`.\n",
    "   - The function returns the `out` variable.\n",
    "\n",
    "2. `if is_infer:`\n",
    "   - This condition checks if the variable `is_infer` is `True`. If it is `True`, it means that the code is being executed in inference mode.\n",
    "\n",
    "3. Inside the `if is_infer:` block:\n",
    "   - The code imports the `optiver2023` module and creates an environment (`env`) for the Optiver 2023 competition.\n",
    "\n",
    "4. It initializes variables, including an iterator (`iter_test`), a counter (`counter`), and variables for specifying the lower and upper limits of predictions (`y_min` and `y_max`).\n",
    "\n",
    "5. It also initializes lists for recording queries per second (`qps`) and a DataFrame (`cache`) to store test data.\n",
    "\n",
    "6. The code enters a loop that iterates through the test data provided by the environment.\n",
    "\n",
    "7. Inside the loop:\n",
    "   - It records the current time (`now_time`) using the `time.time()` function.\n",
    "   - It concatenates the current test data with the existing cache of data.\n",
    "   - It keeps only the most recent 21 rows for each stock and sorts them.\n",
    "   - It generates features for the current test data using the `generate_all_features` function.\n",
    "\n",
    "8. It makes predictions using the previously trained inference LightGBM model (`infer_lgb_model`).\n",
    "\n",
    "9. It applies the `zero_sum` function to transform the predictions.\n",
    "\n",
    "10. It clips the transformed predictions to ensure they fall within the specified range defined by `y_min` and `y_max`.\n",
    "\n",
    "11. It updates the sample prediction with the clipped values.\n",
    "\n",
    "12. It submits the predictions to the environment using the `env.predict()` method.\n",
    "\n",
    "13. It updates the counter and records the time spent on each iteration in the `qps` list.\n",
    "\n",
    "14. It prints the current iteration number and the average queries per second (qps) if the counter is a multiple of 10.\n",
    "\n",
    "15. After processing all test data, it calculates the estimated time cost based on the average qps and prints the estimated time to reason about.\n",
    "\n",
    "This code is designed for making predictions in an Optiver trading competition environment and uses a trained LightGBM model for inference. It also includes a transformation step (`zero_sum`) and clipping of predictions to ensure they are within a specified range before submitting them to the competition environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a7620ae8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-05T13:49:28.398160Z",
     "iopub.status.busy": "2023-11-05T13:49:28.397405Z",
     "iopub.status.idle": "2023-11-05T13:50:49.538158Z",
     "shell.execute_reply": "2023-11-05T13:50:49.537159Z"
    },
    "papermill": {
     "duration": 81.161979,
     "end_time": "2023-11-05T13:50:49.540180",
     "exception": false,
     "start_time": "2023-11-05T13:49:28.378201",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This version of the API is not optimized and should not be used to estimate the runtime of your code on the hidden test set.\n",
      "10 qps: 0.527379035949707\n",
      "20 qps: 0.4981227993965149\n",
      "30 qps: 0.4923568566640218\n",
      "40 qps: 0.4903358221054077\n",
      "50 qps: 0.49753245830535886\n",
      "60 qps: 0.49281309843063353\n",
      "70 qps: 0.4903986556189401\n",
      "80 qps: 0.48869910538196565\n",
      "90 qps: 0.48853297763400605\n",
      "100 qps: 0.4875171947479248\n",
      "110 qps: 0.4863640481775457\n",
      "120 qps: 0.48765721122423805\n",
      "130 qps: 0.4864727790539081\n",
      "140 qps: 0.48697207144328525\n",
      "150 qps: 0.4870727809270223\n",
      "160 qps: 0.4865531548857689\n",
      "The code will take approximately 0.5569 hours to reason about\n"
     ]
    }
   ],
   "source": [
    "def zero_sum(prices, volumes):\n",
    "    std_error = np.sqrt(volumes)\n",
    "    step = np.sum(prices) / np.sum(std_error)\n",
    "    out = prices - std_error * step\n",
    "    return out\n",
    "\n",
    "if is_infer:\n",
    "    import optiver2023\n",
    "    env = optiver2023.make_env()\n",
    "    iter_test = env.iter_test()\n",
    "    counter = 0\n",
    "    y_min, y_max = -64, 64\n",
    "    qps, predictions = [], []\n",
    "    cache = pd.DataFrame()\n",
    "\n",
    "    # Weights for each fold model\n",
    "    model_weights = [1/len(models)] * len(models) \n",
    "    \n",
    "    for (test, revealed_targets, sample_prediction) in iter_test:\n",
    "        now_time = time.time()\n",
    "        cache = pd.concat([cache, test], ignore_index=True, axis=0)\n",
    "        if counter > 0:\n",
    "            cache = cache.groupby(['stock_id']).tail(21).sort_values(by=['date_id', 'seconds_in_bucket', 'stock_id']).reset_index(drop=True)\n",
    "        feat = generate_all_features(cache)[-len(test):]\n",
    "\n",
    "        # Generate predictions for each model and calculate the weighted average\n",
    "        lgb_predictions = np.zeros(len(test))\n",
    "        for model, weight in zip(models, model_weights):\n",
    "            lgb_predictions += weight * model.predict(feat)\n",
    "\n",
    "        lgb_predictions = zero_sum(lgb_predictions, test['bid_size'] + test['ask_size'])\n",
    "        clipped_predictions = np.clip(lgb_predictions, y_min, y_max)\n",
    "        sample_prediction['target'] = clipped_predictions\n",
    "        env.predict(sample_prediction)\n",
    "        counter += 1\n",
    "        qps.append(time.time() - now_time)\n",
    "        if counter % 10 == 0:\n",
    "            print(counter, 'qps:', np.mean(qps))\n",
    "\n",
    "    time_cost = 1.146 * np.mean(qps)\n",
    "    print(f\"The code will take approximately {np.round(time_cost, 4)} hours to reason about\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 5505.464735,
   "end_time": "2023-11-05T13:50:51.295602",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-11-05T12:19:05.830867",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
